\documentclass[11pt, letterpaper]{article}
\usepackage[utf8]{inputenc}
\usepackage[letterpaper, margin=0.5in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{listings}
\usepackage[font=scriptsize]{caption}
\usepackage{subcaption}
\usepackage{xcolor}
\graphicspath{ {.} }
\captionsetup{justification=raggedright, singlelinecheck=false}

\author{Ryan Tang}
\title{STA 602 HW 8}
\date{October 28th 2022}

\begin{document}
\maketitle

\section{Exercise 6.2}
\paragraph{(1) Glucose KDE}
The empirical distribution follows a "somewhat" normal shape. But it is skewed to the right.
\begin{figure*}[!h]
  \centering
  \includegraphics[width=0.7\textwidth]{6.2.1.png}
  \captionsetup{justification=centering}
  \caption{KDE vs Normal Assumption Comparison}
\end{figure*}

\paragraph{(2) Full conditionals}
We are given the following priors and hierarchical model.
\begin{align*}
    Y_i|X_i=k &\thicksim \mathcal{N}(\theta_k, \sigma^2_k) && k \in \{1, 2\} \\
    \theta_k &\thicksim \mathcal{N}(\mu_o, \tau_o^2) \\
    1/\sigma^2_k = \gamma_k &\thicksim Gamma(\frac{\nu_o}{2}, \frac{\nu_o\sigma_o^2}{2}) \\
    X_i &\thicksim Bernoulli(p) \\
    p &\thicksim Beta(a, b)
\end{align*}

Then we can write the following full conditionals regarding the joint in proportionality. And the full conditionals are decomposable in our setup. 
\begin{align*}
    N_k &= \sum_i^N \mathbb{I}(X_i=k) \\
    p(Y, \theta, \sigma^2, X, p) &= p(Y|\theta, \sigma^2, X, p) p(\theta|X,p) p(\sigma^2|X,p) p(X|p) p(p) \\
        &= p(p)
           \prod_i^{N_1} p(Y_i|\theta_1, \sigma^2_1)p(\theta_1)p(\sigma^2_1)p(X_i=1|p)
           \prod_i^{N_2} p(Y_i|\theta_2, \sigma^2_2)p(\theta_2)p(\sigma^2_2)p(X_i=2|p)
\end{align*}

In their full conditionals, $X_i$ and $p$ are still Bernoulli and Beta distributions.
\begin{align*}
    p(p|Y, \theta, \sigma^2, X) &\propto p(Y, \theta, \sigma^2, X, p) \\
        &\propto p(X|p)p(p) \\
        &\propto p^{a-1}(1-p)^{b-1} p^{N_1}(1-p)^{N_2} \\
        &\propto p^{a+N_1-1}(1-p)^{b+N_2-1} \\
        &\thicksim Beta(a+N_1, b+N_2) \\ \\
    p(X|Y, \theta, \sigma^2, p) &\propto \prod_i^N p(Y_i|\theta, \sigma^2, X_i, p) p(X_i|p) \\
        &\propto \prod_i^N
            \left[p(Y_i|\theta_1, \sigma^2_1) p(X_i=1|p)\right]^{\mathbb{I}(X_i=1)}
            \left[p(Y_i|\theta_2, \sigma^2_2) p(X_i=2|p)\right]^{\mathbb{I}(X_i=2)} \\
    p(X_i=1|Y_i, \theta, \sigma^2, p) &\propto p(Y_i|\theta_1, \sigma^2_1) p \\
        &\thicksim Bernoulli\left(
            p_1 = 
            \frac{\mathcal{N}(Y_i|\theta_1, \sigma^2_1) p}
                 {\mathcal{N}(Y_i|\theta_1, \sigma^2_1) p + \mathcal{N}(Y_i|\theta_2, \sigma^2_2) (1-p)}
        \right) \\
    p(X_i=2|Y_i, \theta, \sigma^2, p) &\propto p(Y_i|\theta_2, \sigma^2_2) (1-p) \\
        &\thicksim Bernoulli\left(
            p_2 = 
            \frac{\mathcal{N}(Y_i|\theta_2, \sigma^2_2) (1-p)}
                 {\mathcal{N}(Y_i|\theta_1, \sigma^2_1) p + \mathcal{N}(Y_i|\theta_2, \sigma^2_2) (1-p)}
        \right) \\
        &= 1 - p(X_i=1|Y_i, \theta, \sigma^2, p)
\end{align*}

$\theta_k$ and $\sigma_k$ are the normal semi-conjugate posterior forms in their full conditionals. However, the sufficient statistics are in terms of $N_1$ and $N_2$ that depend on the hidden $X$.
\begin{align*}
    p(\theta_1|Y, \sigma^2, X, p)
        &\propto p(\theta_1) \prod_i^{N_1} p(Y_i|\theta_1, \sigma^2_1) 
            && N_k \bar{Y}_k = \sum_i^N \mathbb{I}(X_i=k) Y_i \\
        &\thicksim \mathcal{N}(
            \mu_{N_1} = 
                \frac{\tilde{\tau}_o}{\tilde{\tau}_{N_1}} \mu_o + \frac{N_1\bar{Y_1}}{\tilde{\tau_{N_1}}}\gamma_1,
            \tilde{\tau}_{N_1} = \tilde{\tau_o} + N_1\gamma_1
        ) \\
    p(\theta_2|Y, \sigma^2, X, p)
        &\thicksim \mathcal{N}(
            \mu_{N_2} = 
                \frac{\tilde{\tau}_o}{\tilde{\tau}_{N_2}} \mu_o + \frac{N_2\bar{Y_2}}{\tilde{\tau_{N_2}}}\gamma_2,
            \tilde{\tau}_{N_2} = \tilde{\tau_o} + N_2\gamma_2
        ) \\
    p(1/\sigma_1^2|Y, \theta, X, p) &= p(\gamma_1|Y, \theta_1, X, p) \\
        &\thicksim Gamma(\frac{\nu_{N_1}}{2}, \frac{\nu_{N_1}\sigma_{N_1}^2}{2}) \\
        &\nu_{N_1} = \nu_o + N_1, \, \sigma_{N_1}^2 = \frac{1}{\nu_{N_1}}(\nu_o\sigma_o^2+\sum_{i\in N_1}(Y_i-\theta_1)^2) \\
    p(1/\sigma_2^2|Y, \theta, X, p) &= p(\gamma_2|Y, \theta_2, X, p) \\
        &\thicksim Gamma(\frac{\nu_{N_2}}{2}, \frac{\nu_{N_2}\sigma_{N_2}^2}{2}) \\
        &\nu_{N_2} = \nu_o + N_2, \, \sigma_{N_2}^2 = \frac{1}{\nu_{N_2}}(\nu_o\sigma_o^2+\sum_{i\in N_2}(Y_i-\theta_2)^2) \\
\end{align*}

\paragraph{(3) Gibbs Samples}
Here we plot the autocorrelation for the two $\theta$ samples, and we can see very strong AR between samples. The effective sample size (ESS) for $\theta_{(1)}^{(s)}$ is $971$ out of 10,000 draws. And the ESS for $\theta_{(2)}^{(s)}$ is $1,202$. The chain is probably not that well mixed after all.
\begin{figure*}[!h]
  \centering
  \includegraphics[width=0.8\textwidth]{6.2.2(1).png}
  \captionsetup{justification=centering}
  \caption{$\theta_{(1)}^{(s)}$ The min of two $\theta$ Autocorrelation Plot }
\end{figure*}

\begin{figure*}[!h]
  \centering
  \includegraphics[width=0.8\textwidth]{6.2.2(2).png}
  \captionsetup{justification=centering}
  \caption{$\theta_{(2)}^{(s)}$ The max of two $\theta$ Autocorrelation Plot }
\end{figure*}

\paragraph{(d) Posterior Predictive}
From the surface, the posterior distribution doesn't quite match the observations. However, I think it is not the mixture model's fault, but Gibbs sampler didn't do a good job exploring the entire sample space. If we use EM instead, the solution might well be better. 
\begin{figure*}[!h]
  \centering
  \includegraphics[width=0.6\textwidth]{6.2.3.png}
  \captionsetup{justification=centering}
  \caption{$\tilde{Y}$ Posterior Predictive Plot}
\end{figure*}

\newpage

\section{Exercise 6.3}
\paragraph{(a) Full Conditional when $c$ is known}
We are given the following hierarchical model in the probit regression with a known $c$.
\begin{align*}
    Z_i &= \beta X_i + \epsilon_i \\
    Y_i &= \delta_{c, \infty}(Z_i) = \mathbb{I}(Z_i>c) 1 + \mathbb{I}(Z_i\leq c) 0 \\
    \epsilon_i &\mathop{\thicksim}^{iid} \mathcal{N}(0, 1) \\
    \beta &\thicksim \mathcal{N}(0, \tau^2_{\beta, o})
\end{align*}
\begin{align*}
    p(Y, X, Z, c, \beta) &= p(Y|Z,c) p(Z|X,\beta) p(\beta) \\
        &= p(\beta) \prod_i^N p(Y_i|Z_i,c) p(Z_i|X_i, \beta) \\ \\
    p(\beta | Y, X, Z, c) &\propto p(\beta) \prod_i^N p(Z_i|X_i, \beta) \\
        &\propto \exp \left[ -\frac{1}{2} [
            \beta^2\tilde{\tau}_{\beta, o} + \sum_i^N(Z_i - X_i\beta)^2
        ]\right] && \frac{1}{\tau_{\beta, o}^2} = \tilde{\tau_{\beta, o}} \\
        &\propto \exp \left[ -\frac{1}{2} [
            \beta^2(\tilde{\tau}_{\beta, o} + \sum_i^N X_i^2) - 2\beta \sum_i Z_i X_i
        ] \right] \\
        &\propto \exp \left[ -\frac{1}{2} \tilde{\tau}_{\beta, N} [
            \beta^2 - 2\beta \frac{\sum_i Z_i X_i}{\tilde{\tau}_{\beta, N}}
        ] \right] && \tilde{\tau}_{\beta, N} = \tilde{\tau}_{\beta, o} + \sum_i^N X_i^2 \\
        &\thicksim \mathcal{N} \left( \frac{\sum_i Z_i X_i}{\tilde{\tau}_{\beta, N}}, \tau^2_{\beta, N} \right)
\end{align*}

\paragraph{(b) Full Conditional for $c$ and $Z_i$}
We can see $c$ is a constrained normal distribution under a interval.
\begin{align*}
    p(c) = \mathcal{N}(0, \tau_c^2) \\
    p(c|Y, X, Z, \beta) &\propto p(Y|Z, c) p(c) \\
        &\propto p(c) \prod_i p(Y_i|Z_i, c) \\
        &\propto p(c) \prod_{i \in Y_i=1} \delta_{(c,\infty)}(Z_i)^{Y_i} \prod_{j \in Y_i=0} \delta_{(-\infty,c)}(Z_j)^{1-Y_j} \\
        &\propto \mathcal{N}(0, \tau_c^2) \mathbb{I}(\max\{Z_i \in Y_i=0\} \leq c < \min\{Z_i \in Y_i=1\}) 
\end{align*}
And on the other hand, the full conditional of $Z_i$ is a truncated normal. First, because we know the latent variable $Z_i$ itself is given by a normal generative model with $X_i$ and $\beta$. But the additional information from $Y_i$ constrained the region that $Z_i$ could be. Hence, the following formulations.
\begin{align*}
    p(Z_i|X_i,Y_i,\beta,c) &\propto p(Y_i|Z_i, c) p(Z_i|X_i, \beta) \\ \\
    p(Z_i|X_i,\beta,Y_i=1,c) &\propto p(Y_i=1|Z_i, c) p(Z_i|X_i, \beta) \\
        &\propto \mathcal{N}(Z_i|X_i\beta, 1)\delta_{(c,\infty)}(Z_i) \\
    p(Z_i|X_i,\beta,Y_i=2,c) &\propto \mathcal{N}(Z_i|X_i\beta, 1)\delta_{(-\infty,c)}(Z_i) \\
\end{align*}

\paragraph{(c) Gibbs Samples}
Here we implemented the above formulation using Gibbs sampler. I had it ran for 2,000 burn-in epochs and additional 30,000 sampling epochs. The following chain is the result. We still see high autocorrelation for both $c$ and $\beta$ during the sampling. The effective sample size for $c$ is 1,156 and it is 1,651 for $\beta$. The chains were very sticky.
\begin{figure*}[!h]
  \centering
  \includegraphics[width=1.0\textwidth]{6.3.3(1).png}
  \captionsetup{justification=centering}
  \caption{$c$ Posterior Gibbs Samples}
\end{figure*}

\begin{figure*}[!h]
  \centering
  \includegraphics[width=1.0\textwidth]{6.3.3(2).png}
  \captionsetup{justification=centering}
  \caption{$\beta$ Posterior Gibbs Samples}
\end{figure*}

\paragraph{(d) $\beta$ Inference}
Lastly, we like to know if the age differences between men and woman would lead to higher divorce rate. Base on the observation data, we have our $\beta|Y,X$ posterior's 95\% confidence interval between $[0.104, 0.673]$. And, the $Pr[\beta > 0|Y,X] = 0.9991$

\end{document}